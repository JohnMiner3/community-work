Topic:

Staging data for Azure SQL Services


Abstract:

Most companies are faced with the ever-growing big data problem. It is estimated that there will be 40 zettabytes of new data generated between 2012 to 2020. Most of this data will be generated by sensors and machines. However, only a small portion of the data is available for users. 

How can IT professionals help business lines gather and process data from various sources? 

There have been two schools of thought on how to solve this problem. 

Schema on write is represented by the traditional relational database. Raw data is ingested by an extract, transform and load (ETL) process. The data is stored in tables that enforce integrity and allow for quick retrieval. Only a small portion of the total data owned by the company resides in the database. 

Schema on read is represented by technologies such as Hadoop or PolyBase. These technologies assumed that data integrity was applied during the generation of the text files. The actual definition of the table is applied during the read operation. All data owned by the company can reside in simple storage. 

Today, we will learn how to stage data using Azure Blob Storage and/or Azure Data Lake Storage. This staged data can be ingested by both techniques.


Coverage:

1 - Grab some big data.
2 - Create blob storage account.
3 - Copy data to container.
4 - Azure SQL database plumbing.
5 - Loading data with BULK INSERT.
6 - Azure SQL data warehouse plumbing.
7 - Loading data with POLYBASE.
8 - Azure automation with RUNBOOKS.


Details:

presentation bundle - Included